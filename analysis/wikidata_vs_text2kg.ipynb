{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "import pandas as pd\n",
        "\n",
        "from utils.structured_aligner import Aligner\n",
        "\n",
        "\n",
        "from pymongo import MongoClient\n",
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_head_triplets(entity_id):\n",
        "    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
        "    query = f\"\"\"\n",
        "    SELECT ?subjectLabel ?propertyLabel ?objectLabel ?object   WHERE {{\n",
        "\n",
        "      SERVICE wikibase:label {{ \n",
        "        bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\" .\n",
        "      }}\n",
        "      VALUES (?subject) {{(wd:{entity_id})}}  \n",
        "      ?subject ?predicate ?object .\n",
        "      ?property wikibase:directClaim ?predicate.\n",
        "\n",
        "      FILTER(STRSTARTS(STR(?predicate), \"http://www.wikidata.org/prop/direct/\")) .\n",
        "      FILTER(STRSTARTS(STR(?object), \"http://www.wikidata.org/entity/\")) .\n",
        "\n",
        "    }}\n",
        "    \"\"\"\n",
        "    \n",
        "    sparql.setQuery(query)\n",
        "    sparql.setReturnFormat(JSON)\n",
        "    results = sparql.query().convert()\n",
        "\n",
        "    output_triplets = []\n",
        "\n",
        "    for result in results[\"results\"][\"bindings\"]:\n",
        "        obj_id = result['object']['value'].split(\"/\")[-1]\n",
        "        subject = result[\"subjectLabel\"][\"value\"]\n",
        "        predicate = result[\"propertyLabel\"][\"value\"]\n",
        "        object_ = result[\"objectLabel\"][\"value\"]\n",
        "    \n",
        "        output_triplets.append({\"subject\": subject, \"predicate\": predicate, \"object\": object_, \"subj_id\": entity_id,\"obj_id\": obj_id})\n",
        "    \n",
        "    return output_triplets\n",
        "\n",
        "get_head_triplets(\"Q19837\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_tail_triplets(entity_id):\n",
        "    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
        "    query = f\"\"\"\n",
        "    SELECT ?subjectLabel ?propertyLabel ?objectLabel ?subject WHERE {{\n",
        "\n",
        "      SERVICE wikibase:label {{ \n",
        "        bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\" .\n",
        "      }}\n",
        "\n",
        "      VALUES (?object) {{(wd:{entity_id})}}  \n",
        "      ?subject ?predicate ?object .\n",
        "      ?property wikibase:directClaim ?predicate.\n",
        "\n",
        "      FILTER(STRSTARTS(STR(?predicate), \"http://www.wikidata.org/prop/direct/\")) .\n",
        "      FILTER(STRSTARTS(STR(?object), \"http://www.wikidata.org/entity/\")) .\n",
        "\n",
        "    }}\n",
        "    \"\"\"\n",
        "    \n",
        "    sparql.setQuery(query)\n",
        "    sparql.setReturnFormat(JSON)\n",
        "    results = sparql.query().convert()\n",
        "    \n",
        "    output_triplets = []\n",
        "    for result in results[\"results\"][\"bindings\"]:\n",
        "        subject = result[\"subjectLabel\"][\"value\"]\n",
        "        predicate = result[\"propertyLabel\"][\"value\"]\n",
        "        object_ = result[\"objectLabel\"][\"value\"]\n",
        "        subj_id = result['subject']['value'].split(\"/\")[-1]\n",
        "\n",
        "        output_triplets.append({\"subject\": subject, \"predicate\": predicate, \"object\": object_, \"subj_id\": subj_id, \"obj_id\": entity_id})\n",
        "    \n",
        "    return output_triplets\n",
        "\n",
        "get_tail_triplets(\"Q19837\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# jobs_triplets = get_head_triplets(\"Q19837\") + get_tail_triplets(\"Q19837\")\n",
        "jobs_triplets = get_head_triplets(\"Q19837\")\n",
        "jobs_df = pd.DataFrame(jobs_triplets).drop_duplicates()\n",
        "jobs_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "coll = db.get_collection('triplets')\n",
        "coll.delete_many({\"sample_id\": 'wikidata-triplets'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Mongo Setup ---\n",
        "_ = load_dotenv(find_dotenv())\n",
        "mongo_client = MongoClient(os.getenv(\"MONGO_URI\"))\n",
        "db = mongo_client.get_database(\"wikidata_ontology\")\n",
        "\n",
        "# --- Extractor Setup ---\n",
        "# extractor = LLMTripletExtractor(model='gpt-4.1-mini')\n",
        "aligner = Aligner(db)\n",
        "\n",
        "\n",
        "jobs_triplets = [{'subject': triple['subject'], 'object': triple['object'], 'relation': triple['predicate'],\n",
        "                    \"subject_type\": None, \"object_type\": None, \"sample_id\": \"wikidata-triplets\"} for triple in jobs_triplets]\n",
        "aligner.add_triplets(jobs_triplets, sample_id='wikidata-triplets')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# apple_triplets = get_head_triplets(\"Q312\") + get_tail_triplets(\"Q312\")\n",
        "apple_triplets = get_head_triplets(\"Q312\")\n",
        "apple_df = pd.DataFrame(apple_triplets).drop_duplicates()\n",
        "apple_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "woznyak_triplets = get_head_triplets(\"Q483382\") + get_tail_triplets(\"Q483382\")\n",
        "woznyak_df = pd.DataFrame(woznyak_triplets).drop_duplicates()\n",
        "woznyak_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pixar_triplets = get_head_triplets(\"Q127552\") + get_tail_triplets(\"Q127552\")\n",
        "pixar_df = pd.DataFrame(pixar_triplets).drop_duplicates()\n",
        "pixar_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "next_triplets = get_head_triplets(\"Q308993\") + get_tail_triplets(\"Q308993\")\n",
        "next_df = pd.DataFrame(next_triplets).drop_duplicates()\n",
        "next_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_df = pd.concat([jobs_df, apple_df, woznyak_df, pixar_df, next_df])\n",
        "all_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_df = all_df.drop_duplicates()\n",
        "all_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def wikidata_id2wikipedia_name(ids):\n",
        "    \n",
        "    num_batches = len(ids) // 50 + int(len(ids) % 50 != 0)\n",
        "    names = {}\n",
        "\n",
        "    for batch in range(num_batches):\n",
        "        id_batch = ids[batch*50:batch*50+50]\n",
        "        id_batch = \"|\".join(id_batch)\n",
        "        res = requests.get(\"https://www.wikidata.org/w/api.php?action=wbgetentities&format=json&props=sitelinks&ids={}&sitefilter=enwiki\".format(id_batch)).json()\n",
        "        for entity in res['entities']:\n",
        "            if \"sitelinks\" in res['entities'][entity] and \"enwiki\" in res['entities'][entity][\"sitelinks\"]:\n",
        "                names[entity] = res['entities'][entity][\"sitelinks\"][\"enwiki\"][\"title\"]\n",
        "    return names\n",
        "\n",
        "def get_alternative_labels(entity_id):\n",
        "    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
        "    query = f\"\"\"\n",
        "    SELECT ?item ?itemAltLabel WHERE {{\n",
        "\n",
        "    VALUES (?subject) {{(wd:{entity_id})}}  \n",
        "    ?subject skos:altLabel ?itemAltLabel . FILTER (lang(?itemAltLabel) = \"en\")\n",
        " \n",
        "\n",
        "    }}\n",
        "    \"\"\"\n",
        "    \n",
        "    sparql.setQuery(query)\n",
        "    sparql.setReturnFormat(JSON)\n",
        "    results = sparql.query().convert()\n",
        "    results = results[\"results\"][\"bindings\"]\n",
        "\n",
        "    output_labels = []\n",
        "    for res in results:\n",
        "        output_labels.append(res['itemAltLabel']['value'])\n",
        "    \n",
        "    return output_labels\n",
        "get_alternative_labels(\"Q483382\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ids = list(set(list(all_df['subj_id']) + list(all_df['obj_id'])))\n",
        "wikidata_id2wiki_mapping = wikidata_id2wikipedia_name(ids)\n",
        "wikidata_id2wiki_mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "wikidata_id2alternative_name = {}\n",
        "for id_ in ids:\n",
        "    wikidata_id2alternative_name[id_] = get_alternative_labels(id_)\n",
        "    time.sleep(0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(wikidata_id2alternative_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wikidata_id2alternative_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_df['wiki_subj'] = all_df['subj_id'].apply(lambda x: wikidata_id2wiki_mapping[x] if x in wikidata_id2wiki_mapping else None)\n",
        "all_df['wiki_obj'] = all_df['obj_id'].apply(lambda x: wikidata_id2wiki_mapping[x] if x in wikidata_id2wiki_mapping else None)\n",
        "\n",
        "all_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_df = all_df.dropna()\n",
        "all_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_df.to_csv(\"wikidata_df.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('full_triplets.csv', index_col=0)\n",
        "df = df.reset_index(drop=True)\n",
        "# df = df.drop_duplicates()\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wikipedia_subjects = list(df.subject.unique())\n",
        "wikipedia_objects = list(df.object.unique())\n",
        "wikipedia_entities = list(set(wikipedia_subjects + wikipedia_objects))\n",
        "\n",
        "wikipedia_relations = list(df.relation.unique())\n",
        "len(wikipedia_entities), len(wikipedia_relations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "aligned_triplets = []\n",
        "for _, row in all_df.iterrows():\n",
        "\n",
        "    triplet_alternative = {\"subject\": row[\"subject\"], \"relation\": row[\"predicate\"], \"object\": row[\"object\"]}\n",
        "    \n",
        "    subject_alternatives = wikidata_id2alternative_name[row['subj_id']] + [row['wiki_subj'], row['subject']]\n",
        "    \n",
        "    for name in subject_alternatives:\n",
        "        if name in wikipedia_entities:\n",
        "            triplet_alternative['subject'] = name\n",
        "            break\n",
        "\n",
        "    object_alternatives = wikidata_id2alternative_name[row['obj_id']] + [row['wiki_obj'], row['object']]\n",
        "\n",
        "    for name in object_alternatives:\n",
        "        if name in wikipedia_entities:\n",
        "            triplet_alternative['object'] = name\n",
        "            break\n",
        "    aligned_triplets.append(triplet_alternative)\n",
        "\n",
        "aligned_df = pd.DataFrame(aligned_triplets)\n",
        "aligned_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wikidata_subjects = list(aligned_df.subject.unique())\n",
        "wikidata_objects = list(aligned_df.object.unique())\n",
        "wikidata_entities = list(set(wikidata_subjects + wikidata_objects))\n",
        "\n",
        "wikidata_relations = list(aligned_df.relation.unique())\n",
        "len(wikidata_entities), len(wikidata_relations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparing with the composed KG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = \"cuda:5\"\n",
        "tokenizer = AutoTokenizer.from_pretrained('facebook/contriever')\n",
        "model = AutoModel.from_pretrained('facebook/contriever').to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mean_pooling(token_embeddings, mask):\n",
        "    token_embeddings = token_embeddings.masked_fill(~mask[..., None].bool(), 0.)\n",
        "    sentence_embeddings = token_embeddings.sum(dim=1) / mask.sum(dim=1)[..., None]\n",
        "    return sentence_embeddings\n",
        "\n",
        "\n",
        "def embed_batch(names):\n",
        "    inputs = tokenizer(names, padding=True, truncation=True, return_tensors='pt')\n",
        "    outputs = model(**inputs.to(device))\n",
        "    embeddings = mean_pooling(outputs[0], inputs['attention_mask'])\n",
        "    return np.array(embeddings.detach().cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embed_batch([\"Steve Jobs\", \"Stephen Woznyak\"]).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_df.to_csv(\"wikidata_df.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(set(wikidata_entities) & set(wikipedia_entities))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wikipedia_entities_embedded = embed_batch(wikipedia_entities)\n",
        "wikipedia_entities_embedded.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wikipedia_relations_embedded = embed_batch(wikipedia_relations)\n",
        "wikipedia_relations_embedded.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wikidata_entities_embedded = embed_batch(wikidata_entities)\n",
        "wikidata_entities_embedded.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wikidata_relations_embedded = embed_batch(wikidata_relations)\n",
        "wikidata_relations_embedded.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "relation_similarity_matrix = metrics.pairwise.cosine_similarity(wikidata_relations_embedded, wikipedia_relations_embedded)\n",
        "relation_similarity_matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "entity_similarity_matrix = metrics.pairwise.cosine_similarity(wikidata_entities_embedded, wikipedia_entities_embedded)\n",
        "entity_similarity_matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_entity_pairs = np.argmax(entity_similarity_matrix, axis=0)\n",
        "len(best_entity_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pedia2data_entity = {}\n",
        "best_entity_pairs = np.argmax(entity_similarity_matrix, axis=0)\n",
        "\n",
        "for i, _ in enumerate(wikipedia_entities):\n",
        "    if entity_similarity_matrix[best_entity_pairs[i]][i] > 0.5:\n",
        "        pedia2data_entity[wikipedia_entities[i]] = wikidata_entities[best_entity_pairs[i]]\n",
        "pedia2data_entity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(pedia2data_entity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pedia2data_relation = {}\n",
        "best_relation_pairs = np.argmax(relation_similarity_matrix, axis=0)\n",
        "\n",
        "for i, _ in enumerate(wikipedia_relations):\n",
        "    if relation_similarity_matrix[best_relation_pairs[i]][i] > 0.5:\n",
        "        pedia2data_relation[wikipedia_relations[i]] = wikidata_relations[best_relation_pairs[i]]\n",
        "pedia2data_relation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(pedia2data_relation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "triplet_pairs = []\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    data_subj = pedia2data_entity[row['subject']] if row['subject'] in pedia2data_entity else None \n",
        "    data_obj = pedia2data_entity[row['object']] if row['object'] in pedia2data_entity else None\n",
        "    data_rel = pedia2data_relation[row['relation']] if row['relation'] in pedia2data_relation else None\n",
        "\n",
        "    if data_subj and data_obj and data_rel:\n",
        "        triplet = aligned_df[(aligned_df['subject'] == data_subj) & (aligned_df['object'] == data_obj) & (aligned_df['relation'] == data_rel)]\n",
        "        if len(triplet) > 0:\n",
        "            triplet_pairs.append(((row['subject'], row['relation'], row['object']), (triplet.iloc[0,0], triplet.iloc[0,1], triplet.iloc[0,2])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(triplet_pairs), triplet_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wikidata_triplets = list(aligned_df['subject'] +  \" \" + aligned_df['relation'] + \" \" + aligned_df['object'])\n",
        "wikipedia_triplets =  list(df['subject'] +  \" \" + df['relation'] + \" \" + df['object'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wikidata_triplets[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wikipedia_triplets[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "set(wikipedia_triplets) & set(wikidata_triplets), len(set(wikipedia_triplets) & set(wikidata_triplets))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wikidata_triplets_embedded = embed_batch(wikidata_triplets)\n",
        "wikidata_triplets_embedded.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wikipedia_triplets_embedded = embed_batch(wikipedia_triplets)\n",
        "wikipedia_triplets_embedded.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "triplet_similarity_matrix = metrics.pairwise.cosine_similarity(wikidata_triplets_embedded, wikipedia_triplets_embedded)\n",
        "triplet_similarity_matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_triplet_pairs = np.argmax(triplet_similarity_matrix, axis=0)\n",
        "best_triplet_pairs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "triplet_pairs = []\n",
        "\n",
        "for i, _ in enumerate(wikipedia_triplets):\n",
        "    if triplet_similarity_matrix[best_triplet_pairs[i]][i] > 0.5:\n",
        "        triplet_pairs.append((wikipedia_triplets[i], wikidata_triplets[best_triplet_pairs[i]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(triplet_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "aligned_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "aligned_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparing with linked names from wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pairs = []\n",
        "pair_count = 0\n",
        "edge_present = 0\n",
        "\n",
        "counted_pairs = set()\n",
        "\n",
        "wikipedia_common_triplets = []\n",
        "wikidata_common_triplets = []\n",
        "\n",
        "\n",
        "for _, row in aligned_df.iterrows():\n",
        "\n",
        "    subj_name = row['subject']\n",
        "    obj_name = row['object']\n",
        "\n",
        "\n",
        "    if subj_name in wikipedia_entities and obj_name in wikipedia_entities:\n",
        "        wikidata_common_triplets.append((row['subject'], row['relation'], row['object']))\n",
        "        # edge_present += len(df[((df['subject'] == row['wiki_subj']) & (df['object'] == row['wiki_obj'])) | ((df['subject'] == row['wiki_obj']) & (df['object'] == row['wiki_subj']))])\n",
        "        if (subj_name, obj_name) in counted_pairs or (obj_name, subj_name) in counted_pairs:\n",
        "            continue\n",
        "        else:\n",
        "            edge_present += 1\n",
        "            counted_pairs.add((subj_name, obj_name))\n",
        "\n",
        "            intersected_triplets = df[((df['subject'] == subj_name) & (df['object'] == obj_name)) | ((df['subject'] == obj_name) & (df['object'] == subj_name))]\n",
        "\n",
        "            if len(intersected_triplets) > 0:\n",
        "                pair_count += 1\n",
        "                for _, row_ in intersected_triplets.iterrows():\n",
        "                    # wikidata_common_triplets.append((row['wiki_subj'], row['predicate'], row['wiki_obj']))\n",
        "                    wikipedia_common_triplets.append((row_['subject'], row_['relation'], row_['object']))\n",
        "                    # print(row['wiki_subj'], row['predicate'], row['wiki_obj'])\n",
        "                    # print(row_['subject'], row_['relation'], row_['object'])\n",
        "                    # print()\n",
        "                # pair_count += len(intersected_triplets)\n",
        "            \n",
        "pair_count, pair_count/edge_present, edge_present"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"Up\" in wikipedia_entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[df['subject'] == 'Up']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for pedia_triplet, data_triplet in zip(wikipedia_common_triplets, wikidata_common_triplets):\n",
        "    print(pedia_triplet, \" | \", data_triplet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Neo4j"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from neo4j import GraphDatabase\n",
        "\n",
        "uri = \"neo4j://localhost:7687\"\n",
        "username = \"neo4j\"\n",
        "password = \"12345678\"\n",
        "driver = GraphDatabase.driver(uri, auth=(username, password))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def add_node(tx, node_name):\n",
        "#     tx.run(\"CREATE (n:WikidataNode {name: $node_name})\", node_name=node_name)\n",
        "\n",
        "# def add_relation(tx, head, tail, relation):\n",
        "#     query = f\"\"\"\n",
        "#         MATCH (a {{name: $head}}), (b {{name: $tail}})\n",
        "#         CREATE (a)-[r:{relation}]->(b)\n",
        "#         RETURN type(r)\n",
        "#         \"\"\"\n",
        "#     result = tx.run(query, head=head, tail=tail, database_='abc')\n",
        "\n",
        "# def get_node(tx, name):\n",
        "#     result = tx.run(\"MATCH (n:WikidataNode {name: $name}) RETURN n.name AS name\", name=name)\n",
        "#     return [record[\"name\"] for record in result]\n",
        "\n",
        "\n",
        "# for i, row in all_df.iterrows():\n",
        "#     head = row['subject']\n",
        "#     tail = row['object']\n",
        "#     relation = \"_\".join(row['predicate'].replace(\"-\", \"\").replace(\"/\", \"\").replace(\"'\", \"\").replace(\",\", \"\").replace(\".\", \"\").split())\n",
        "#     # print(head, tail, relation)\n",
        "#     with driver.session() as session:\n",
        "#         if not session.read_transaction(get_node, head):\n",
        "#             session.write_transaction(add_node, head)\n",
        "\n",
        "#         if not session.read_transaction(get_node, tail):\n",
        "#             session.write_transaction(add_node, tail)\n",
        "            \n",
        "#         session.write_transaction(add_relation, head, tail, relation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_node(tx, node_name):\n",
        "    tx.run(\"CREATE (n:WikidataNode {name: $node_name})\", node_name=node_name)\n",
        "\n",
        "def add_relation(tx, head, tail, relation):\n",
        "    query = f\"\"\"\n",
        "        MATCH (a {{name: $head}}), (b {{name: $tail}})\n",
        "        CREATE (a)-[r:{relation}]->(b)\n",
        "        RETURN type(r)\n",
        "        \"\"\"\n",
        "    result = tx.run(query, head=head, tail=tail, database_='abc')\n",
        "\n",
        "def get_node(tx, name):\n",
        "    result = tx.run(\"MATCH (n:WikidataNode {name: $name}) RETURN n.name AS name\", name=name)\n",
        "    return [record[\"name\"] for record in result]\n",
        "\n",
        "\n",
        "for triplet in wikidata_common_triplets:\n",
        "    head = triplet[0]\n",
        "    tail = triplet[2]\n",
        "    relation = \"_\".join(triplet[1].replace(\"-\", \"\").replace(\"/\", \"\").replace(\"'\", \"\").replace(\",\", \"\").replace(\".\", \"\").split())\n",
        "    # print(head, tail, relation)\n",
        "    with driver.session() as session:\n",
        "        if not session.read_transaction(get_node, head):\n",
        "            session.write_transaction(add_node, head)\n",
        "\n",
        "        if not session.read_transaction(get_node, tail):\n",
        "            session.write_transaction(add_node, tail)\n",
        "            \n",
        "        session.write_transaction(add_relation, head, tail, relation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_node(tx, node_name):\n",
        "    tx.run(\"CREATE (n:WikipediaNode {name: $node_name})\", node_name=node_name)\n",
        "\n",
        "def add_relation(tx, head, tail, relation):\n",
        "    query = f\"\"\"\n",
        "        MATCH (a:WikipediaNode {{name: $head}}), (b:WikipediaNode {{name: $tail}})\n",
        "        CREATE (a)-[r:{relation}]->(b)\n",
        "        RETURN type(r)\n",
        "        \"\"\"\n",
        "    result = tx.run(query, head=head, tail=tail, database_='abc')\n",
        "\n",
        "def get_node(tx, name):\n",
        "    result = tx.run(\"MATCH (n:WikipediaNode {name: $name}) RETURN n.name AS name\", name=name)\n",
        "    return [record[\"name\"] for record in result]\n",
        "\n",
        "\n",
        "for triplet in wikipedia_common_triplets:\n",
        "    head = triplet[0]\n",
        "    tail = triplet[2]\n",
        "    relation = \"_\".join(triplet[1].replace(\"-\", \"\").replace(\"/\", \"\").replace(\"'\", \"\").replace(\",\", \"\").replace(\".\", \"\").split())\n",
        "    # print(head, tail, relation)\n",
        "    with driver.session() as session:\n",
        "        if not session.read_transaction(get_node, head):\n",
        "            session.write_transaction(add_node, head)\n",
        "\n",
        "        if not session.read_transaction(get_node, tail):\n",
        "            session.write_transaction(add_node, tail)\n",
        "            \n",
        "        session.write_transaction(add_relation, head, tail, relation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def delete_all(tx):\n",
        "    tx.run(\"MATCH (n) DETACH DELETE n\")\n",
        "\n",
        "with driver.session() as session:\n",
        "    session.execute_write(delete_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
